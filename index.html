<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    <meta name="description" content="" />
    <meta name="author" content="" />
    <link
            href="https://fonts.googleapis.com/css?family=Lato:100,300,400,700,900"
            rel="stylesheet"
    />

    <title>Anina Camara - diplomová práca</title>
    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet" />

    <!-- Additional CSS Files -->
    <link rel="stylesheet" href="assets/css/fontawesome.css" />
    <link rel="stylesheet" href="assets/css/templatemo-style.css" />
    <link rel="stylesheet" href="assets/css/owl.css" />
    <link rel="stylesheet" href="assets/css/lightbox.css" />
</head>
<body>
<div id="page-wraper">
    <!-- Sidebar Menu -->
    <div class="responsive-nav">
        <i class="fa fa-bars" id="menu-toggle"></i>
        <div id="menu" class="menu">
            <i class="fa fa-times" id="menu-close"></i>
            <div class="container">
                <div class="image">
                    <img src="assets/images/author-image.jpg" alt="author image" />
                </div>
                <div class="author-content">
                    <h4>Bc.Anna Camara</h4>
                    <span>Anina</span>
                </div>
                <nav class="main-nav" role="navigation">
                    <ul class="main-menu">
                        <li class = "active"><a href="index.html" >Diploma thesis</a></li>

                        <ul class="sub-menu">
                            <li><a href="#section1">Abstract</a></li>
                            <li><a href="#section2">Literature</a></li>
                            <li><a href="#section3">Future Plans</a></li>
                        </ul>
                        <li><a href="progress.html"  >Progress Calendar</a></li>
                        <li><a href="Diplomovka_Camara.pdf"  >Current Version</a></li>
                    </ul>
                </nav>
                <div class="copyright-text">
                    <p>Copyright 2019 Reflux Design
                        <br>
                        Adjustments 2022 Anna Camara
                    </p>
                </div>
            </div>
        </div>
    </div>
<!-- ----------------------Abstract-------------------------->
    <section class="section abstract" data-section="section1" id="section1" >
        <div class="container">
            <div class="section-heading">
                <h2>Abstract</h2>
                <div class="line-dec"></div>
                <span>Person identification with partially occluded face.</span>
                <p class="text-left"> The goal of the thesis is to identify a person in case when face is partially
                    occluded for example with sunglasses or face mask. Study the topic of person
                    identification based on the face. Analyze the performance of the existing
                    solutions published in the literature. Propose a new method based on a neural
                    network, which can find and identify a person. Create a dataset for training and
                    testing purposes. Evaluate the proposed method and draw the conclusions.</p>
            </div>
        </div>
        <div class="container">
            <div class="section-heading">
                <h2>Goals</h2>
                <div class="line-dec"></div>
                <p class="text-left">
                <ul class="ul-left">
                    <li>Learn about current approaches in face identification</li>
                    <li>Discover what is true about face identification working worse for people of color. Why and how to fix it.</li>
                    <li>Make/find more gender and race balanced database - probably merge some databases together </li>
                    <li>Focus on face occlusion by face mask.</li>
                    <li>Use face masks only in testing and validation set.</li>
                </ul>
            </div>
        </div>
    </section>

    <!-- ----------------------Literature------------------------ -->
    <section class="section literature" data-section="section2" id="section2">
        <div class="container">
            <div class="section-heading">
                <h2>Literature</h2>
                <div class="line-dec"></div>
                <span>This is all of the literature I read to get familiar with the topic</span>
                <p class="text-left">
                <table>
                    <tr>
                        <th class="article-name">Name of the article</th>
                        <th>link</th>
                        <th>Short descrition</th>
                    </tr>
                    <tr>
                        <td>Improved Performance of Face Recognition using
                            CNN with Constrained Triplet Loss Layer</td>
                        <td><a href="https://ieeexplore-ieee-org.proxy.library.uu.nl/abstract/document/7966089" target="_blank">article</a></td>
                        <td>This paper proposed a new loss layer that can be replaced at the bottom of a neural
                            network architecture in terms of face recognition, called constrained triplet loss layer (CTLL).</td>
                    </tr>
                    <tr>
                        <td>Masked Face Recognition Dataset and Application</td>
                        <td><a href="https://arxiv.org/abs/2003.09093" target="_blank">article</a></td>
                        <td>This work proposes three types of masked face datasets, including Masked Face Detection
                            Dataset (MFDD), Real-world Masked Face Recognition Dataset (RMFRD) and Simulated Masked
                            Face Recognition Dataset (SMFRD). Among them, to the best of our knowledge, RMFRD is
                            currently theworld's largest real-world masked face dataset. These datasets are freely
                            available to industry and academia, based on which various applications on masked faces
                            can be developed.</td>
                    </tr>
                    <tr>
                        <td>Masked Face Recognition Challenge: The WebFace260M Track Report</td>
                        <td><a href="https://arxiv.org/abs/2108.07189" target="_blank">article</a></td>
                        <td>This paper is the official report of WebFace260M Track
                            in the MFR workshop and challenge. We detail the training data, evaluation protocols,
                            submission rules, test set and metric in standard face recognition and masked face recognition
                            , ranking criterion, baseline solution, and preliminary competition results</td>
                    </tr>
                    <tr>
                        <td>Masked Face Recognition Challenge: The InsightFace Track Report</td>
                        <td><a href="https://openaccess.thecvf.com/content/ICCV2021W/MFR/html/Deng_Masked_Face_Recognition_Challenge_The_InsightFace_Track_Report_ICCVW_2021_paper.html" target="_blank">article</a></td>
                        <td>For the InsightFace track, we manually collect a large-scale masked face test set with 7K
                            identities. In addition, we also collect a children test set including 14K identities and
                            a multi-racial test set containing 242K identities. By using these three test sets,
                            we build up an online model testing system, which can give a comprehensive evaluation
                            of face recognition models.</td>
                    </tr>
                    <tr>
                        <td>WebFace260M: A Benchmark Unveiling the Power of Million-Scale Deep Face Recognition</td>
                        <td><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhu_WebFace260M_A_Benchmark_Unveiling_the_Power_of_Million-Scale_Deep_Face_CVPR_2021_paper.pdf" target="_blank">article-database</a></td>
                        <td>In this paper, we contribute a new million-scale face  benchmark containing noisy 4M identities/260M faces
                            (WebFace260M) and cleaned 2M identities/42M faces  (WebFace42M) training data, as well as an elaborately designed time-constrained evaluation protocol. Firstly, we
                            collect 4M name list and download 260M faces from the Internet. Then, a Cleaning Automatically
                            utilizing SelfTraining (CAST) pipeline is devised to purify the tremendous WebFace260M, which
                            is efficient and scalable. To the best of our knowledge, the cleaned WebFace42M is the
                            largest public face recognition training set and we expect to  close the data gap between academia and industry.</td>
                    </tr>
                    <tr>
                        <td>Level Playing Field for Million Scale Face Recognition</td>
                        <td><a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Nech_Level_Playing_Field_CVPR_2017_paper.pdf" target="_blank">article-database</a></td>
                        <td>MF2 dataset was created from Flickr (utility photo-sharing platform) photos. Unlike other
                            mentioned datasets this one consists mostly of non-celebs.</td>
                    </tr>
                    <tr>
                        <td>FaceMask: A New Image Dataset for the Automated Identification of People Wearing Masks in the Wild</td>
                        <td><a href="file:///home/anina/Downloads/sensors-22-00896-v3.pdf" target="_blank">article-database</a></td>
                        <td>We present a publicly available annotated image database containing images of people with and
                            without a mask on their faces, in different environments and situations.</td>
                    </tr>
                    <tr>
                        <td>MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition</td>
                        <td><a href="https://link-springer-com.proxy.library.uu.nl/content/pdf/10.1007/978-3-319-46487-9_6.pdf?pdf=inline%20link" target="_blank">article-database</a></td>
                        <td>In this paper, we design a benchmark task and provide the  associated datasets for recognizing
                            face images and link them to corresponding entity keys in a knowledge base. More specifically, we propose
                            a benchmark task to recognize one million celebrities from their face
                            images, by using all the possibly collected face images of this individual on the web as training data.</td>
                    </tr>
                    <tr>
                        <td>Face Recognition in Unconstrained Videos with Matched Background Similarity</td>
                        <td><a href="http://www.cs.tau.ac.il/~wolf/papers/lvfw.pdf" target="_blank">article-database</a></td>
                        <td>We present a comprehensive database of labeled videos of faces in challenging, uncontrolled
                            conditions (i.e., ‘in the wild’), the ‘YouTube Faces’ database, along with benchmark,
                            pairmatching tests1. (b) We employ our benchmark to survey  and compare the performance
                            of a large variety of existing video face recognition techniques. Finally, (c) we describe
                            a novel set-to-set similarity measure, the Matched  Background Similarity (MBGS).
                            This similarity is shown to considerably improve performance on the benchmark tests.</td>
                    </tr>
                    <tr>
                        <td>Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments</td>
                        <td><a href="https://hal.inria.fr/inria-00321923/" target="_blank">article-database</a></td>
                        <td>This database, Labeled Faces in the Wild, is provided as an aid in studying the latter,
                            unconstrained, recognition problem. The database contains labeled face photographs spanning
                            the range of conditions typically encountered in everyday life. The database exhibits
                            “natural” variability in factors such as pose, lighting, race, accessories, occlusions,
                            and background. In addition to describing the details of the database, we provide specific
                            experimental paradigms for which the database is suitable. </td>
                    </tr>
                    <tr>
                        <td>DeepFace: Closing the Gap to Human-Level Performance in Face Verification</td>
                        <td><a href="https://research.facebook.com/publications/deepface-closing-the-gap-to-human-level-performance-in-face-verification/" target="_blank">article</a></td>
                        <td>In modern face recognition, the conventional pipeline
                            consists of four stages: detect ⇒ align ⇒ represent ⇒ classify.
                            We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to
                            apply a piecewise affine transformation, and derive a face
                            representation from a nine-layer deep neural network.</td>
                    </tr>
                    <tr>
                        <td>Face Recognition Systems: A Survey</td>
                        <td><a href="https://www.mdpi.com/1424-8220/20/2/342" target="_blank">article</a></td>
                        <td>Contribution of this survey is to review some well-known techniques for each approach
                            and to give the taxonomy of their categories. In the paper, a detailed comparison between
                            these techniques is exposed by listing the advantages and the disadvantages of their schemes
                            in terms of robustness, accuracy, complexity, and discrimination</td>
                    </tr>
                    <tr>
                        <td>Thales: Facial recognition: top 7 trends (tech, vendors, use cases)</td>
                        <td><a href="https://www.thalesgroup.com/en/markets/digital-identity-and-security/government/biometrics/facial-recognition" target="_blank">website</a></td>
                        <td>Overview on current facial recognition techniques</td>
                    </tr>
                    <tr>
                        <td>Biometrics Institute: What is Biometrics?</td>
                        <td><a href="https://www.biometricsinstitute.org/what-is-biometrics/" target="_blank">website</a></td>
                        <td>Definition of biometrics</td>
                    </tr>
                    <tr>
                        <td>Biometrics Institute: Types of Biometrics: Face – Use Cases</td>
                        <td><a href="https://www.biometricsinstitute.org/types-of-biometrics-face-use-cases/" target="_blank">website</a></td>
                        <td>Examples of face verification and identification.</td>
                    </tr>
                    <tr>
                        <td>Past, Present, and Future of Face Recognition: A Review</td>
                        <td><a href="https://www.mdpi.com/2079-9292/9/8/1188" target="_blank">article</a></td>
                        <td>This review presents the history of face recognition technology, the current
                            state-of-the-art methodologies, and future directions. We specifically concentrate on
                            the most recent databases, 2D and 3D face recognition methods.</td>
                    </tr>
                    <tr>
                        <td>An anatomical and photographic technique for forensic facial identification</td>
                        <td><a href="https://www-sciencedirect-com.proxy.library.uu.nl/science/article/pii/S0379073800002905" target="_blank">article</a></td>
                        <td>This paper describes a photographic technique which allows accurate anatomical measurement
                            and tracing of facial features, which allows direct physical comparison of ID document images.</td>
                    </tr>
                    <tr>
                        <td>ISO/IEC 2382-37:2022(en) Information technology — Vocabulary — Part 37: Biometrics</td>
                        <td><a href="https://www.iso.org/obp/ui/#iso:std:iso-iec:2382:-37:ed-3:v1:en:term:37.08.03" target="_blank">website</a></td>
                        <td>Definition of biometrics.</td>
                    </tr>
                    <tr>
                        <td>FaceFilter: Face Identification with Deep Learning and Filter Algorithm</td>
                        <td><a href="https://www.hindawi.com/journals/sp/2020/7846264/" target="_blank"> article </a></td>
                        <td>In this study, we present a system that can directly identify an individual under all
                            conditions (faces are in different poses or have different levels of illumination,
                            or when the face is blurred) by extracting the most important features and using them to
                            identify a person. Our method uses a deep convolutional network that is trained to extract
                            the most important features. A filter is then used to ...</td>
                    </tr>
                    <tr>
                        <td> Masked face identification is improved by diagnostic feature training </td>
                        <td><a href="https://cognitiveresearchjournal.springeropen.com/articles/10.1186/s41235-022-00381-x#citeas"
                                                   target="_blank">article</a></td>
                        <td>Human performance</td>
                    </tr>
                    <tr>
                        <td>Face Recognition Performance: Role of Demographic Information</td>
                        <td><a href="https://www.researchgate.net/publication/260299859_Face_Recognition_Performance_Role_of_Demographic_Information" target="_blank">article </a></td>
                        <td>This paper studies the influence of demographics on the performance of face recognition
                            algorithms. The recognition accuracies of six different face recognition algorithms
                            (three commercial, two nontrainable, and one trainable) are computed on a large scale
                            gallery that is partitioned so that each partition consists entirely of specific demographic cohorts. </td>
                    </tr>
                    <tr>
                        <td>Gender Shades: Intersectional Accuracy Disparities in
                            Commercial Gender Classification</td>
                        <td><a href="http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf" target="_blank"> article </a></td>
                        <td>In this work, we present an approach to evaluate bias present in automated facial analysis
                            algorithms and datasets with respect to phenotypic subgroups</td>
                    </tr>
                    <tr>
                        <td>Is facial recognition technology worse at identifying darker-skinned faces than lighter ones?</td>
                        <td><a href="https://www.abc.net.au/news/2020-02-04/fact-check-facial-recognition-darker-skin/11781192?nw=0&r=Interactive" target="_blank">article</a></td>
                        <td>Commercial article - ...</td>
                    </tr>

                </table>
            </div>
        </div>
    </section>
    
        <section class="section plany" data-section="section3" id="section3" >
        <div class="container">
            <div class="section-heading">
                <h2>Future plans</h2>
                <div class="line-dec"></div>
                <span>To do list. What I am planning to read or what I'mm planning to get back to.</span>
                <p class="text-left">
                <ul class="ul-left">
                    <li>read the paper about database <a href="https://github.com/joojs/fairface">FairFace</a> </li>
                    <li>learn about <a href = "https://arxiv.org/pdf/1503.03832.pdf">FaceNet</a> and <a href = "https://github.com/akshaybahadur21/Facial-Recognition-using-Facenet">models using it</a></li>
                    <li>learn about <a href = "https://github.com/TadasBaltrusaitis/OpenFace">OpenFace</a> </li>
                    <li>learn about triplet loss function</li>
                    <li>learn about face recognition without triplet loss function like they do in <a href = "https://www.hindawi.com/journals/sp/2020/7846264/"> here </a></li>
                    <li>learn about current face recognition systems - analyse their performance on masked faces and write a chpter about it <li>
                </ul>
            </div>
        </div>
    </section>

</div>
<script src="vendor/jquery/jquery.min.js"></script>
<script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

<script src="assets/js/isotope.min.js"></script>
<script src="assets/js/owl-carousel.js"></script>
<script src="assets/js/lightbox.js"></script>
<script src="assets/js/custom.js"></script>
<script>
    //according to loftblog tut
    // $(".main-menu li:first").addClass("active");

    var showSection = function showSection(section, isAnimate) {
        var direction = section.replace(/#/, ""),
            reqSection = $(".section").filter(
                '[data-section="' + direction + '"]'
            ),
            reqSectionPos = reqSection.offset().top - 0;

        if (isAnimate) {
            $("body, html").animate(
                {
                    scrollTop: reqSectionPos
                },
                800
            );
        } else {
            $("body, html").scrollTop(reqSectionPos);
        }
    };

    var checkSection = function checkSection() {
        $(".section").each(function() {
            var $this = $(this),
                topEdge = $this.offset().top - 80,
                bottomEdge = topEdge + $this.height(),
                wScroll = $(window).scrollTop();

            if (topEdge < wScroll && bottomEdge > wScroll) {
                var currentId = $this.data("section"),
                    reqLink = $("a").filter("[href*=\\#" + currentId + "]");
                reqLink
                    .closest("li")
                    .addClass("active")
                    .siblings()
                    .removeClass("active");
            }
        });
    };

    $(".sub-menu").on("click", "a", function(e) {
        e.preventDefault();
        showSection($(this).attr("href"), true);
    });

    $(window).scroll(function() {
        checkSection();
    });
</script>
</body>
</html>
